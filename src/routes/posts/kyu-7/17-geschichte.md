+++
title = "7. Kyū"
color = "#fff"
created = 2024-02-25
+++

<script lang="ts">
  import Figure from '$lib/components/Figure.svelte';
</script>

## Geschichte

Geschichte in einem Programmierkurs? Ja, es gibt Menschen, die bedeutendes in der Informatik geleistet haben. So bedeutend, dass man ihre Namen kennen sollte. Auch im Umfeld des Chaos Computer Clubs werden sie dir immer wieder begegnen. Nach ihnen werden bei Veranstaltungen Räume oder Plätze benannt oder sogar eigene Briefmarken für die [Chaospost](https://chaospost.de/) gedruckt. In jedem Gurt wollen wir dir ein paar dieser Menschen vorstellen. Und historische Hardware wird später noch hinzutreten. Wenn du quasi einen Rundumschlag über die Geschichte rund um Computer und Internet haben willst, dann genieße das Werk von Walter Isaacson „The Innovators“. Das beschreibt sehr gut sowohl die technische Entwicklung als auch die Menschen dahinter. Berlinerinnen bekommen das Buch in der Berliner Stadtbibliothek unter der Signatur Inf 25/38 leihweise. In anderen Städten frag mal bei deiner Bibliothek nach - sonst lohnt da auch der Kauf bzw. es auf die Geschenkeliste setzen zu lassen.

Zuvorderst aber kommen wir kurz zum Begriff Computer. Der stammt nicht direkt aus dem Englischen, wie man vermuten könnte. Der Ursprung ist lateinisch und kommt von computare, was berechnen heißt. Der Begriff findet sich in gedruckter Form das erste mal 1892 in der _New York Times_. Dort wurden _Computer_ gesucht. Das war seinerzeit noch ein Beruf, bei dem Menschen Berechnungen mit Zettel und Stift durchführten. Beispielsweise haben Sternwarten händisch von _Computern_ berechnen lassen, welche Position die Planeten zu einem bestimmten Zeitpunkt haben. Diese menschlichen Computer waren oft - Frauen. Als Rechenmaschinen aufkamen wurden deren Leistungen anfangs in „Girl-Years“ gemessen und Einheit wurde der Begriff „Kilogirl“ verwendet. Die Geschichte der Informatik baut also auf Frauen auf - wie wir auch gleich bei unserem ersten Star sehen werden.

### Ada Lovelace

> Ich bin als Prophetin in die Welt geboren worden
>
> und diese Überzeugung erfüllt mich mit
>
> Demut, Zittern und Beben.
>
> Ada Lovelace

Die erste Programmierin lebte, bevor es überhaupt Computer gab: Die britische Mathematikerin Ada King-Noel, Countess of Loveloce, wurde 1815 geboren. Countess entspricht dem deutschen Titel „Gräfin“. Ihr Vater war der seinerzeit berühmte Dichter Lord Byron. Ihren Vater lernte Ada nie persönlich kennen, da sich die Eltern früh trennten. Die Trennung ging von ihrer Mutter aus, was zur damaligen Zeit ein Skandal war. Ihre Mutter gehörte dem Adel an und war dadurch vermögend. Zu Adas Zeit durften Frauen in Großbritannien noch nicht studieren (verrückt, oder?). Ihre Mutter legte jedoch großen Wert auf Bildung. Durch ihr Vermögen konnte sie Ada täglichen Privatunterricht zugute kommen lassen. An sechs Tagen in der Woche wurde Ada in Mathematik, Geografie, Astronomie, Französisch, Deutsch und Latein unterrichtet. Mathe-Unterricht erhielt sie bereits ab dem 4. Lebensjahr. Untericht in Literatur, Philosophie und Dichtkunst unterband die Mutter. Sie wollte verhindern, dass Ada nach ihrem Vater kam. Die Liebe zur Literatur und Dichtung von Ada verhinderte sie damit aber nicht. Sonntags wurde sie in der Bauen und Formen nach [Pestalozzi](https://de.wikipedia.org/wiki/Johann_Heinrich_Pestalozzi) geschult. Architektur wäre vielleicht eine passende Beschreibung für den Inhalt dieses Unterrichts.

<Figure src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Ada_Lovelace.jpg/220px-Ada_Lovelace.jpg" alt="Ada Lovelace" float="right" />

Trotz oder vielleicht aufgrund ihrer Ausbildung entwickelte Ada ein hohes Interesse an technischen und mathematischen Fragestellungen - ohne ihre Zuneigung zur Poesie zu verlieren. Ada lernte Charles Babbage kennen, als sie 17 und er 42 Jahre alt war. Dieser entwickelte eine mechanische Rechenmaschine. Damit wollte er die aufwändigen und fehleranfälligen Berechnungen von Tafelwerken für Astronomie und Nautik, also Seefahrt, vereinfachen. 1841 lud Charles Babbage Ada Lovelace ein, um von seiner Maschine zu berichten. Ada nutzte in der Mathematik bereits dreidimensionale Modelle und detaillierte großflächige Zeichnungen, um sich mathematische Sachverhalte zu verdeutlichen. Dies nutzte sie in ihren folgenden Entwicklungen. Anfangs sollte sie eine Übersetzung eines Artikels von Babbage erstellen. Das Original versah sie nicht nur mit zahlreichen Notizen, sondern er wurde etwa dreimal länger als das Original. Dabei entwickelte sie einen umfangreichen Algorithmus für die Berechnung der [Bernoulli-Zahlen](https://de.wikipedia.org/wiki/Bernoulli-Zahl#Reihen_mit_Bernoulli-Zahlen). Damit hat Ada 1843 den ersten Algorithmus entwickelt - noch bevor es Computer oder Programmiersprachen gab. Wie ein solcher Algorithmus entwickelt werden kann, schauen wir uns später auch einmal an.

Die Rechenmaschine von Babbage wurde übrigens nie gebaut - es fehlte zum Schluss schlichtweg das Geld.

Zu Ehren Ada Lovelaces wurde nach ihr die [gleichnamige Programmiersprache Ada](<https://de.wikipedia.org/wiki/Ada_(Programmiersprache)>) benannt.

Ada starb 1852 als Mutter von drei Kindern mit gerade 36 Jahren an [Gebärmutterhalskrebs](https://de.wikipedia.org/wiki/Zervixkarzinom#Impfung). Die häufigste Ursache für diese Krebsform sind [Humane Papillomaviren](https://de.wikipedia.org/wiki/Humane_Papillomviren). Dagegen gibt es mittlerweile eine Impfung, die für männliche (!) und weibliche Jugendliche empfohlen wird. [Infos hierzu findest du unter anderem hier](https://www.impfen-info.de/impfempfehlungen/fuer-jugendliche-12-17-jahre/hpv-humane-papillomaviren/).

Leider nur noch [auf YouTube findet sich dieser](https://www.youtube.com/watch?v=3cBYZEKjOao) sehenswerte drei-Minuten-Beitrag von Arte zu Lovelace.

Auf jeden Fall hörenswert zu Ada Lovelace ist die [18-Minuten-Folge](https://sphinx.acast.com/starke-frauen/adalovelace-dieerfinderindesprogrammierens/media.mp3?tk=eyJ0ayI6ImRlZmF1bHQiLCJhZHMiOnRydWUsInNwb25zIjp0cnVlLCJzdGF0dXMiOiJwdWJsaWMifQ==&sig=ZJ8hGzytk2wrrPJZrzOOMuRodIXoN7s1pzMahMr1Jb4) aus dem Podcast „Starke Frauen“.

Und wenn du noch tiefer eintauchen willst, dann lohnt sich ein Blick in das Buch „Ada Lovelace - die Pionierin der Computertechnik und ihre Nachfolgerinnen“, welches von Sybille Krämer herausgegeben wurde. Berliner:innen können dieses Buch in der [Berliner Stadtbibliothek](https://www.zlb.de/), Nähe Alexanderplatz, unter der Signatur Inf 25/30 ausleihen.

Falls du mehr über die Rechenmaschine von Charles Babbages erfahren möchtest, findest Du [hier einen Vortrag](https://media.ccc.de/v/vcfb20_-_137_-_de_-_202010101600_-_the_programming_architecture_of_babbages_analytical_engine_-_prof_dr_raul_rojas#t=9) von Prof. Rojas, welcher auf dem Vintage Computer Festival Berlin 2020 gehalten wurde.

Ehrungen und Projekte mit Bezug auf Ada Lovelace gibt es diverse. Erwähnt sei an dieser Stelle nur das [Promotionsprogramm der Universität Münster](https://www.uni-muenster.de/GleichstellungFB10/foerdermassnahmen/ada-lovelace-promotionsstellen/index.html). Vielleicht kommt dieses Programm ja für dich mal in Betracht.

Bildnachweis: [Margaret Sarah Carpenter, Public domain, via Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Ada_Lovelace.jpg?uselang=de)

### Alan Turing

> Programmieren ist eine Fähigkeit,
>
> die am besten durch Übung und Auszuprobieren
>
> und nicht aus Büchern erworben wird.
>
> Alan Turing

Der Brite Alan Turing wurde 1912 geboren und starb 1954. „Er gilt heute als einer der einflussreichsten Theoretiker der frühen Computerentwicklung und Informatik,“ schreibt die [Wikipedia](https://de.wikipedia.org/wiki/Alan_Turing). Wenn du eine großartige Darstellung seines hoch tragischen Lebens sehen willst - mach einen Moment Pause und besorge dir den etwas zweistündigen Film [„The Imitation Game“](https://www.youtube.com/watch?v=KsJH9HWhgCw). Wir gehen davon aus, dass du diesen Film genießt und geben hier nur noch ein paar Randdetails wieder.

<Figure src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Alan_Turing_in_watercolour.png/240px-Alan_Turing_in_watercolour.png" alt="Alan Turing" float="right" />

Alan fiel von klein auf an als besonders begabt auf. Während ihm Naturwissenschaften sehr zusagten, konnte er mit Geisteswissenschaften nichts anfangen. Da nicht ganz einsah, für diese Fächer genau so viel zu tun, wie für die Naturwissenschaften, musste er sogar das College wechseln. Seiner erfolgreichen Beschäftigung mit Mathematik tat dies keinen Abbruch. Alan war übrigens ein großer Bewunderer von Charles Babbage, den du bei Alan Lovelace kennen gelernt hast. Er entwarf die sogenannte [Turingmaschine](https://de.wikipedia.org/wiki/Turingmaschine). Hierdurch bewies er, dass jedes mathematisches Problem gelöst werden kann, wenn es durch einen Algorithmus gelöst werden kann. Die Turingmaschine stellt bis heute in der theoretischen Informatik einen Schwerpunkt dar. Das wird zwar noch eine ganze Weile dauern, aber wir haben vor, uns diesem Thema einmal zu nähern.

Die Nazis verwendeten zur Codierung ihrer Funksprüche im Zweiten Weltkrieg an beispielsweise U-Boote die sogenannte [Enigma-Maschine](<https://de.wikipedia.org/wiki/Enigma_(Maschine)>). Dieser Code galt aufgrund seiner Komplexität lange Zeit als unknackbar. Alan Turing war maßgeblich daran beteiligt, dass dieser Code gebrochen wurde und der Krieg eine erhebliche Wendung nahm.

Alan lehrte später an der Universität Manchester. Dort beschäftigte er sich auch mit künstlicher Intelligenz. Um festzustellen, ob ein Computer eine dem Menschen ebenbürtiges Denkvermögen habe, schlug er 1950 den sogenannten [Turing-Test](https://de.wikipedia.org/wiki/Turing-Test) vor. Er selbst nannte den Test „Imitation Game“. Vereinfacht gesagt, soll dieser dann ein bestanden gelten, wenn ein Mensch mit einem Computer nur über Chat kommuniziert und nicht mehr feststellen kann, ob es sich um einen Computer oder Menschen handelt. Turing vermutete, dass im Jahr 2000 dieses Ziel erreicht sein. Bis heute ist es nicht gelungen, ein System zu programmieren, welches diesen Test besteht.

Eines der ersten Schachprogramme verdanken wir ebenso Alan Turing.

Homosexualität war zu Turings Zeit in Großbritannien (und auch in Deutschland) eine Straftat. Alan war schwul und deswegen 1952 strafrechtlich verurteilt. Er wurde chemisch kastriert, war zu einer Depression führte. In der Folge starb Alan Turing etwa zwei Jahre später durch Suizid. Im Jahre 2009 sprach der britische Premierminister Gordon Brown eine offizielle Entschuldigung für die „entsetzliche Behandlung“ aus und würdigte Alan Turings „außerordentliche Verdienste“ während des Krieges aus. Eine Begnadigung wurde trotzdem zunächst abgelehnt, bis Queen Elisabeth II. 2013 eine königliche Begnadigung aussprach.

Der [Turing-Award](https://de.wikipedia.org/wiki/Turing_Award) ist quasi der Nobelpreis der Informatik. Das dieser nach ihm benannt wurde, spricht für sich.

Auf den Film haben wir schon verwiesen. Lesenswert ist das Comic von Robert Deutsch „Turing“, welches du auch in zahlreichen Bibliotheken erhälst.

Als Roman greift Neal Stephenson in seinem Werk „Cryptonomicon“ die Kryptologie der Enigma-Maschine auf und nimmt Bezüge auf Alan Turing. Neal Stephenson wird dir noch öfter über den Weg laufen. Der Roman lohnt sich auf jeden Fall.

Bildnachweis: [Midjourney AI, prompted by Netha Hussain, Public domain, via Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Alan_Turing_in_watercolour.png?uselang=de)

:::tip Info
Dieses Kapitel ist gerade im Entstehen... hab bitte noch einen Moment Geduld, bis es vollständig ist.
:::

### Grace Hopper

> Der gefährlichste Satz einer Sprache ist:
>
> „Das haben wir schon immer so gemacht“.
>
> Grace Hopper

<Figure src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Grace_Hopper.jpg/205px-Grace_Hopper.jpg" alt="Grace Hopper" float="right" />

Rückblickend kann man sich ja manchmal kaum vorstellen, dass bestimmte Dinge jemals anders waren. Und es benötigt Menschen vom Schlag wie Grace Hopper, um sie zu ändern: Bis dahin war es üblich, in kaum verständlicher Maschinensprache zu programmieren. Hopper sorgte dafür, dass Programme in „verständlicher“ Sprache geschrieben wurden und entwickelte 1951 bis 1952 den ersten Compiler: [A-0](https://de.wikipedia.org/wiki/A-0). Mit einem Compiler werden Befehle, wie du sie schon in diesem Kurs kennen gelernt hast, in Maschienensprache übersetzt - am Ende lange Ketten aus Nullen und Einsen. Das war wirklich geheimnisvoller Code.

Die US-Amerikanerin Grace Hopper wurde 1906 in New York geboren. Sie studierte Mathematik und Physik und war die elfte Frau, die an der Yale-Universität promovierte, also einen Doktortitel erwarb. Später hat sie maßgebliche Vorarbeiten zur Programmiersprache [COBOL](https://de.wikipedia.org/wiki/COBOL) geleistet. Dadurch erwarb sie sich den Spitznamen „Grandma COBOL“. COBOL hat sich auf die Fahnen geschrieben, dass sein Syntax möglichst nah an die natürliche, menschlichen Sprache angelehnt ist. COBOL wurde insbesondere im kaufmännischen Bereich eingesetzt und findet sich heute noch auf den Systemen von Banken und Versicherungsen.

Grace Hopper war für die Harvard-Universität tätig, bevor sie zum US-Militär wechselte. Dort diente sie bis zu ihrem 60. Lebensjahr. Nur ein Jahr später, 1967, wurde sie wieder in den aktiven Dienst versetzt und verblieb bis zu ihrem 80. Lebensjahr. Sie verließ die US-Marine als Admiral der US Navy Reserve. Das entspricht einem Generalsrang. Ihre Mitarbeiter nannten sie „Amazing Grace“ (engl. unglaubliche Grace). Das lag sicherlich nicht nur an den Computerproblemen, für deren Lösung sie weiterbeschäftigt wurde. So demonstrierte Grace beispielsweise gerne ihre Beidhändigkeit bei Vorträgen - sie begann einen Satz mit der linken Hand auf Deutsch, um ihn in der Mitte der Tafel mit der rechten Hand auf Französisch fortzusetzen. Mit ihrem Ausscheiden aus dem militärischen Dienst war ihre aktive Tätigkeit nicht vorbei, sondern sie arbeite weiterhin als Beraterin für ein privates Unternehmen.

<Figure src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/First_Computer_Bug%2C_1945.jpg/304px-First_Computer_Bug%2C_1945.jpg" alt="Erster Bug" float="right" />

Der Begriff Bug für einen Fehler stammt zwar wohl nicht von Grace Hopper, aber ihr Debugging, also der Begriff für die Suche nach Fehlern und deren Beseitigung, wird ihr gerne zugeschrieben. 1947 verirrte sich eine Motte in ein Relais des von ihr betreuten Computers. Aus Relais bestanden die seinerzeit die Rechner, bevor diese durch Chips ersetzt wurden. Die Motte sorgte für den Ausfall eines Relais und entsprechendes Unheil. Es handelt sich also nicht um einen Software-, sondern einen Hardwarefehler. Trotzdem passte es ganz gut. Die Motte klebte Grace in ihr Logbuch ein und verfasste dazu den Satz:

> „First actual case of bug being found.“
>
> (engl. „Das erste Mal,
> dass tatsächlich ein Käfer/Bug gefunden wurde.“

Grace Hopper starb 1992 im Bundesstaat Virginia. Ihr Werk wurde verschiedentlich gewürdigt: Nach Hopper ist inzwischen ein Komet benannt, in der brandenburgischen Stadt Teltow trägt eine Gesamtschule ihren Namen, Präsident Obama verlieh ihr 2016 posthum die [„Presidential Medal of Freedom”](https://de.wikipedia.org/wiki/Presidential_Medal_of_Freedom), eine der höchsten zivilen Auszeichnungen der Vereinigten Staaten und vieles mehr. Beispielsweise wurde nach ihr auch der [Grace Murray Hopper Award](https://de.wikipedia.org/wiki/Grace_Murray_Hopper_Award) benannt. Dieser mit 35.000 Dollar dotierte Preis wird an Computerexperten verliehen, die zum Zeitpunkt der Verleihung nicht älter als 35 Jahre als sind. Einigen der Preisträger wie Donald Knuth werden dir später noch vorstellen.

Wenn du Lust hast, dich tiefer mit Grace Hopper zu beschäftigten, sei dir „Die Pionierinnen des Internets - Die unbekannte Geschichte der Frauen des digitalen Zeitalters“ von Claire Evans ans Herz gelegt. Berlinerinnen erhalten es in der Berliner Stadtbibliothek in der Breite Straße unter der Signatur Inf 40/57 auch leihweise. Das Buch ist unabhängig von Hopper sehr empfehlenswert. Sie ist ja nicht die einzige Frau, um die es dort geht.

Es ist erschreckend zu sehen, wie man mit Frauen lange Zeit umgegangen ist und wieviel wir in der Informatik Frauen zu verdanken haben, obwohl es als männliches Fach erscheint. Das lässt den Blick auf heute kritisch schärfen und reflektieren. Das Buch ist also ganz klar nicht nur als Mutmacher für Menschen empfohlen, die sich nicht dem männlichen Geschlecht zuordnen. Gerade Männer sollten es lesen. Es ist wirklich gut und kurzweilig zu lesen... also nur zu!

### Konrad Zuse

Wir schreiben über Menschen und wir werden an dieser Stelle sehen, dass das nicht immer einfach ist. Und das nicht, weil unklar ist, ob ihm zurecht zugeschrieben wird, den Computer erfunden zu haben. Das Eingangs erwähnte Buch von Walter Isaacson schildert das wunderbar. Hier erfolgt nur die Kurzfassung.

Um sagen zu können, wer den Computer erfunden hat, wäre zunächst die Frage zu beantworten, was denn ein Computer überhaupt ist. Wikipedia sagt, dass es ein Gerät ist, „das mittels programmierbarer Rechenvorschriften Daten verarbeitet. [...] synonym gebrauchte Begriffe [...] eleketronische Datenverarbeitungsanlage“.

Georg Stibitz hat ein sogenanntes K-Modell 1937 auf seinem Küchentisch angefangen zu bauen, welches er 1940 im Labor des amerikanischen Telefonunternehmens Bell Labs Januar 1940 fertig stellte. Der war zwar binär, aber noch mit elektromechanischen Relais ausgestattet. Stellt man darauf ab, dass das Gerät komplett elektronisch arbeiten muss, reicht das noch nicht.

Konrad Zuse hat zunächst den Z1 entwickelt. Falls du in Berlin wohnst oder mal zu Besuch bist - gehe unbedingt ins [Technik Museum](https://technikmuseum.berlin/). Da steht nämlich ein sehenswerter Nachbau. Aber der war noch richtig mechanisch. So in richtig elektronisch wurde erst der im Januar 1941 fertigstellte Z3. Der war jedoch nicht als Universalrechner konzipiert, sondern um technische Berechnungen anzustellen. Die brauchte man damals massenhaft und das war sehr mühsam. Zu der Zeit war Krieg und die Berechnungen waren unter anderem für den Flugzeugbau und für die Berechnung von Flugbahnen von Geschossen wichtig. Die Z3 wurde 1943 während eines Bombenangriffs auf Berlin zerstört. Eine Gedenktafel in Berlin-Kreuzberg erinnert heute noch daran. Dadurch ist der Rechner nie richtig in Betrieb gegangen. Es konnte jedoch später gezeigt werden, dass obwohl dafür nicht konstruiert dieser als Universalrechner hätte dienen können.

Und der dritte, der Anspruch auf die Lorbeeren geltend machen könnte, ist Vincent Atanasoff. Das größte Manko war, dass er im September 1942 die Arbeit daran aufgab, um zur Marine zu gehen. Der Rechner funktionierte zu diesem Zeitpunkt nocht nicht vollständig und verwendete mechanische Elemente zur Speicherung. Am Ende verschwand das gute Stück im Keller der Iowa State University.

Im Dezember 1943 haben Max Newman und Tommy Flowers unter Beteiligung von Alan Turing einen Rechner entworfen, der digital und vollelektronisch war. Aber er war kein Universialrechner und damit nicht sogenannt turingmächtig.

Howard Aiken nahm im Mai 1944 in Kooperation mit IBM den Mark I in Betrieb. Der war programmierbar, aber elektromechanisch. Tja.

Und Presper Eckert und John Mauchly stellten im November 1945 den ENIAC fertig. Der erfüllte alle genannten Anforderungen und zu allem Glück - der funktionierte sogar.

Und trotzdem wird beispeilsweise Konrad Zuse in der [Wikipedia](https://de.wikipedia.org/wiki/Konrad_Zuse) als der Erbauer des ersten funktionsfähigen Computers der Welt bezeichnet. Gleichwie - die Zeit war offensichtlich reif dafür. Nur ist erklärbar, dass mehrere Menschen ohne unbedingt von den Ergebnissen der anderen zu wissen, Computer erfanden. Und so gab es nach dem zweiten Weltkrieg auch einen längeren Patentstreit unter anderem gegen IBM, bei dem Zuse verlor. Erst in den 70er Jahren wurde ihm der heutige Ruhm zuteil. Und trotz der langen Vorrede, wer zu der Zeit noch fleißig dabei war ist das Werk Zuses nicht gering zu schätzen. Er baute in seinem Leben 251 Rechenmaschinen und hat die Entwicklung von Computer maßgeblich mit vorangebracht. Sein Unternehmen, die [Zuse KG](https://de.wikipedia.org/wiki/Zuse_KG) konnte jedoch nicht lange überleben und ging im späteren Siemens-Konzern auf. Wer wissenschaftlich arbeitet und große Rechenkapazitäten benötigt - beispielsweise im Bereich Klimaforschung - kommt am Hochleistungsrechenzentrum des [Konrad-Zuse-Instituts](https://www.zib.de/de/institut/konrad-zuse) nicht vorbei. In Berin gibt es sogar eine [Schule](https://kzsb.de/ueber-uns/), die nach Konrad Zuse benannt wurde.

Konrad Zuse verfasste eine Autobiographie - „Der Computer - Mein Lebenswerk“. Wer sich näher mit der Person Zuses beschäftigen will, für den lohnt sich das. Zuse hat später auch noch gemalt, was man in dem Werk von Helmut Böttiger - „Konrad Zuse - Erfinder, Unternehmer, Philosoph und Künstler“ bestaunen kann. Was aber dabei untergeht, ist eine dunkle Seite der Geschichte. Lange wurde - auch von Zuse selbst - der Mythos aufrecht erhalten, dass er während der Nazi-Zeit als Wissenschaftler bzw. Ingenieur eine quasi neutrale Stellung behielt. Wie sich aber in späteren Recherchen herausstellte, stand Zuse den Nazis deutlich näher als zunächst bekannt war. Da wurde also lange vieles aus seinem Leben geschickt ausgelassen, insbesondere das er seine Rechner für eine „systematische Rassenforschung“ einsetzen wollte. Der Spiegel hat dies in einem Artikel von 14. Juni 2010, Heft 24, gut zusammengefasst. Im Online-Archiv des Spiegels - welches manche Bibliotheken auch zur Verfügung stellen - kann man den Artikel abrufen. Wie eingangs geschrieben - die Geschichte eines solchen Menschen ist nicht immer einfach.
